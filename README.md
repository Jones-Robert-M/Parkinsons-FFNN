# Parkinson's Disease Prediction - Feedforward Neural Network

A complete PyTorch-based machine learning pipeline for predicting Parkinson's disease progression metrics (motor_UPDRS and total_UPDRS) from voice telemonitoring data. This project demonstrates end-to-end ML workflow including exploratory data analysis, model training, evaluation, and prediction analysis.

## ğŸ¯ Project Overview

This project implements a shallow feedforward neural network for regression tasks on the Parkinson's Telemonitoring dataset. The goal is to predict disease progression scores from voice measurements, providing a non-invasive method for monitoring Parkinson's disease.

### Key Features

- âœ… **Complete ML Pipeline**: Data preprocessing â†’ Model training â†’ Evaluation â†’ Prediction
- âœ… **Comprehensive EDA**: Statistical analysis, PCA, correlation studies, outlier detection
- âœ… **Production-Ready**: Modular code structure with configuration management
- âœ… **Reproducible**: Fixed random seeds, versioned dependencies, documented experiments
- âœ… **Visualization**: Training curves, residual analysis, prediction quality assessment
- âœ… **Best Practices**: Early stopping, learning rate scheduling, proper train/val/test splits

## ğŸ“ Project Structure

```
Parkinsons-FFNN/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ local.yaml              # Local training configuration
â”‚   â””â”€â”€ prod.yaml               # Production configuration
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # Raw dataset files
â”‚   â””â”€â”€ predictions/            # Model predictions output
â”œâ”€â”€ models/                     # Saved models and artifacts
â”‚   â”œâ”€â”€ best_model.pt          # Trained model checkpoint
â”‚   â”œâ”€â”€ scaler.pkl             # Feature scaler
â”‚   â””â”€â”€ training_history.png   # Training curves visualization
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ eda.ipynb              # Exploratory data analysis (12 sections)
â”‚   â””â”€â”€ prediction_analysis.ipynb  # Model prediction analysis (10 sections)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ preprocessing.py    # Data loading and preprocessing
â”‚   â”‚   â””â”€â”€ dataset.py          # PyTorch Dataset class
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ ffnn.py            # Feedforward neural network
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ trainer.py         # Training loop with early stopping
â”‚   â”‚   â””â”€â”€ metrics.py         # Evaluation metrics
â”‚   â”œâ”€â”€ pipelines/
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                # Main training script
â”‚   â”œâ”€â”€ predict.py             # Inference script
â”‚   â””â”€â”€ utils.py               # Helper functions
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ __init__.py            # Unit tests
â”œâ”€â”€ requirements.txt           # Project dependencies
â”œâ”€â”€ CLAUDE.md                  # Project workflow documentation
â””â”€â”€ README.md                  # This file
```

## Dataset

**Parkinson's Telemonitoring Dataset**
- **Instances:** 5,875 voice recordings
- **Features:** 22 (including age, gender, time, and various voice measurements)
- **Targets:** motor_UPDRS and total_UPDRS scores
- **Source:** UCI Machine Learning Repository

## Installation

1. Create and activate virtual environment:
```bash
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

## Usage

### 1. Training

Train the model with default configuration:
```bash
cd src
python main.py
```

Train with custom configuration:
```bash
python main.py --config ../config/local.yaml
```

### 2. Making Predictions

Use the trained model for inference:
```bash
python predict.py --data ../data/raw/parkinsons_updrs.data --output data/prediction/predictions.csv
```

### 3. Exploratory Data Analysis

Explore the dataset with comprehensive statistical analysis:
```bash
jupyter lab notebooks/eda.ipynb
```

**EDA Notebook Contents:**
- Dataset overview and statistics
- Target variable analysis
- Feature distributions
- Correlation analysis
- Principal Component Analysis (PCA)
- PCA component loadings
- 2D/3D PCA visualizations
- Outlier detection (Isolation Forest)
- Pairwise feature relationships
- Time-series analysis
- Demographic analysis
- Key findings and recommendations

### 4. Prediction Analysis

Analyze model predictions and performance:
```bash
jupyter lab notebooks/prediction_analysis.ipynb
```

**Prediction Analysis Contents:**
- Overall performance metrics
- Prediction vs actual scatter plots
- Residual analysis
- Error distribution by prediction range
- Q-Q plots for normality checks
- Per-subject prediction accuracy
- Temporal prediction tracking
- Error correlation analysis
- Feature impact on errors
- Comprehensive summary

## Model Architecture

**Feedforward Neural Network:**
- Input layer: 19 features (after preprocessing)
- Hidden layers: [64, 32, 16] neurons with ReLU activation
- Batch normalization after each hidden layer
- Dropout (0.2) for regularization
- Output layer: 2 neurons (motor_UPDRS, total_UPDRS)

**Training Features:**
- Loss: Mean Squared Error (MSE)
- Optimizer: Adam with weight decay
- Learning rate scheduler: ReduceLROnPlateau
- Early stopping with patience=15
- Train/Val/Test split: 70/10/20

## âš™ï¸ Configuration & Experimentation

The model is highly configurable through YAML files or CLI arguments, making it easy to experiment with different architectures and hyperparameters.

### Configuration Methods

#### 1. YAML Configuration Files

Edit or create YAML files in `config/` directory:

```yaml
# config/local.yaml
data:
  path: "../data/raw/parkinsons_updrs.data"
  target_cols: ["motor_UPDRS", "total_UPDRS"]
  exclude_cols: ["subject#"]
  test_size: 0.2  # 20% test set
  val_size: 0.1   # 10% validation set

model:
  hidden_dims: [64, 32, 16]  # 3 hidden layers
  dropout_rate: 0.2          # 20% dropout
  activation: "relu"         # relu, tanh, elu, leaky_relu

training:
  batch_size: 32
  epochs: 150
  learning_rate: 0.001
  weight_decay: 0.00001
  patience: 15  # Early stopping patience
  seed: 42      # Random seed for reproducibility
```

#### 2. Command-Line Arguments (Override Config)

CLI arguments override YAML settings, enabling quick experiments:

```bash
# View all available options
python main.py --help

# Override learning rate and batch size
python main.py --lr 0.0001 --batch-size 64

# Try different architecture
python main.py --hidden-dims 128 64 32 --dropout 0.3

# Full experiment with custom name
python main.py --config ../config/deep_net.yaml \
               --lr 0.0005 \
               --experiment-name deep_exp1 \
               --output-dir ../experiments/deep

# Quick architecture test
python main.py --hidden-dims 32 16 --epochs 50 --experiment-name quick_test
```

### Available CLI Arguments

**Data Parameters:**
- `--data-path PATH` - Dataset file path
- `--test-size FLOAT` - Test set proportion (0.0-1.0)
- `--val-size FLOAT` - Validation set proportion (0.0-1.0)

**Model Architecture:**
- `--hidden-dims N [N ...]` - Hidden layer sizes (e.g., `--hidden-dims 128 64 32`)
- `--dropout FLOAT` - Dropout rate (0.0-1.0)
- `--activation {relu,tanh,elu,leaky_relu}` - Activation function

**Training Hyperparameters:**
- `--batch-size INT` - Training batch size
- `--epochs INT` - Maximum training epochs
- `--lr, --learning-rate FLOAT` - Learning rate
- `--weight-decay FLOAT` - L2 regularization strength
- `--patience INT` - Early stopping patience
- `--seed INT` - Random seed

**Output:**
- `--output-dir PATH` - Save directory for models
- `--experiment-name NAME` - Experiment name (appended to files)
- `--no-plots` - Disable plot generation

### Pre-configured Experiments

The project includes several ready-to-use configurations:

```bash
# Small network (faster training, fewer parameters)
python main.py --config ../config/small_net.yaml

# Deep network (more capacity)
python main.py --config ../config/deep_net.yaml

# High learning rate experiment
python main.py --config ../config/high_lr.yaml

# Default configuration
python main.py --config ../config/local.yaml
```

### Configuration Examples

**Example 1: Grid Search Over Learning Rates**
```bash
for lr in 0.0001 0.0005 0.001 0.005 0.01; do
  python main.py --lr $lr --experiment-name lr_${lr} --output-dir ../experiments/lr_search
done
```

**Example 2: Architecture Search**
```bash
# Shallow network
python main.py --hidden-dims 32 --experiment-name shallow

# Medium network
python main.py --hidden-dims 64 32 --experiment-name medium

# Deep network
python main.py --hidden-dims 128 64 32 16 --experiment-name deep

# Very deep network
python main.py --hidden-dims 256 128 64 32 16 --experiment-name very_deep
```

**Example 3: Regularization Study**
```bash
# Low dropout
python main.py --dropout 0.1 --weight-decay 0.00001 --experiment-name low_reg

# Medium dropout
python main.py --dropout 0.3 --weight-decay 0.0001 --experiment-name med_reg

# High dropout
python main.py --dropout 0.5 --weight-decay 0.001 --experiment-name high_reg
```

### Saved Experiment Artifacts

Each training run saves:
- `best_model_<name>.pt` - Model checkpoint
- `scaler_<name>.pkl` - Feature scaler
- `training_history_<name>.png` - Loss curves
- `config_<name>.yaml` - Exact configuration used

This enables easy reproducibility and experiment tracking!

ğŸ“˜ **For comprehensive experimentation guide, see [EXPERIMENTS.md](EXPERIMENTS.md)** - includes:
- Complete parameter reference tables
- Common experiment workflows (grid search, architecture search, etc.)
- Troubleshooting guide
- Best practices for systematic experimentation

## Results

After training, the following artifacts are saved to `models/`:
- `best_model.pt` - Model checkpoint with best validation loss
- `scaler.pkl` - StandardScaler for feature normalization
- `training_history.png` - Training/validation loss curves

## ğŸ“Š Evaluation Metrics

The model is evaluated using:
- **MSE** (Mean Squared Error) - Average squared difference between predictions and actuals
- **RMSE** (Root Mean Squared Error) - Square root of MSE, in original units
- **MAE** (Mean Absolute Error) - Average absolute difference
- **RÂ²** (Coefficient of Determination) - Proportion of variance explained (0-1 scale)
- **MAPE** (Mean Absolute Percentage Error) - Percentage-based error metric

## ğŸš€ Expected Results

Based on the trained model:

**Test Set Performance:**
- Motor UPDRS: RÂ² â‰ˆ 0.61, RMSE â‰ˆ 4.97, MAE â‰ˆ 3.85
- Total UPDRS: RÂ² â‰ˆ 0.64, RMSE â‰ˆ 6.26, MAE â‰ˆ 4.75

**Training Details:**
- Epochs: ~78 (early stopping)
- Learning rate: 0.001 â†’ 0.000125 (adaptive)
- Parameters: 4,146 trainable weights
- Training time: ~30 seconds on CPU

## ğŸ”¬ Experimental Workflow

1. **Data Exploration** (`notebooks/eda.ipynb`)
   - Load and analyze the Parkinson's dataset
   - Perform PCA and correlation studies
   - Identify outliers and patterns

2. **Model Training** (`src/main.py`)
   - Preprocess and split data (70/10/20)
   - Train FFNN with early stopping
   - Save best model and artifacts

3. **Prediction Generation** (`src/predict.py`)
   - Load trained model and scaler
   - Generate predictions on dataset
   - Save to `data/predictions/`

4. **Results Analysis** (`notebooks/prediction_analysis.ipynb`)
   - Evaluate prediction quality
   - Analyze residuals and errors
   - Generate insights and recommendations

## ğŸ› ï¸ Development

### Running Tests
```bash
pytest tests/
```

### Modifying Architecture
Edit `config/local.yaml` to experiment with:
- Hidden layer dimensions
- Dropout rates
- Activation functions
- Learning rates
- Batch sizes

### Adding Features
The modular structure allows easy extension:
- New models in `src/models/`
- Custom metrics in `src/training/metrics.py`
- Additional preprocessing in `src/data/preprocessing.py`

## ğŸ“ License

See LICENSE file for details.

## ğŸ™ Acknowledgments

This project demonstrates best practices for:
- Reproducible ML research
- Clean code architecture
- Comprehensive evaluation
- Transparent reporting

## ğŸ“š Citation

**Dataset Source:**
```
A Tsanas, MA Little, PE McSharry, LO Ramig (2009)
'Accurate telemonitoring of Parkinson's disease progression by non-invasive speech tests',
IEEE Transactions on Biomedical Engineering
```

**Repository:**
```
Parkinson's FFNN Project
https://github.com/Jones-Robert-M/Parkinsons-FFNN
```

---

## ğŸ“– Quick Start Guide

```bash
# 1. Setup
git clone https://github.com/Jones-Robert-M/Parkinsons-FFNN
cd Parkinsons-FFNN
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

# 2. Explore data
jupyter lab notebooks/eda.ipynb

# 3. Train model
cd src
python main.py

# 4. Generate predictions
python predict.py --data ../data/raw/parkinsons_updrs.data --output ../data/predictions/predictions.csv

# 5. Analyze results
cd ..
jupyter lab notebooks/prediction_analysis.ipynb
```

**That's it!** You now have a trained neural network with complete analysis. ğŸ‰
